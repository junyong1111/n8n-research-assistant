"""
ë…¼ë¬¸ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ, ìš”ì•½ ì €ì¥ ë“±
"""
from fastapi import APIRouter, HTTPException, status
from pydantic import BaseModel, Field
from typing import Optional, Dict
from pathlib import Path
import json
from datetime import datetime

from app.services.pdf_processor import PDFProcessor
from app.services.semantic_scholar import SemanticScholarService
from app.utils.logger import get_logger

logger = get_logger()
router = APIRouter()


class PaperSummaryRequest(BaseModel):
    """ë…¼ë¬¸ ìš”ì•½ ì €ì¥ ìš”ì²­"""
    paper_id: str = Field(..., description="ë…¼ë¬¸ ID")
    summary: str = Field(..., description="ë…¼ë¬¸ ìš”ì•½ (LLM ìƒì„±)")
    metadata: Optional[Dict] = Field(None, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")


class FindPDFResponse(BaseModel):
    """PDF ì°¾ê¸° ì‘ë‹µ"""
    paper_id: str
    pdf_found: bool
    pdf_url: Optional[str] = None
    source: Optional[str] = None  # "semantic_scholar" | "arxiv" | "unpaywall"
    local_path: Optional[str] = None


class ExtractTextResponse(BaseModel):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‘ë‹µ"""
    paper_id: str
    text_length: int
    num_pages: Optional[int] = None
    text: str
    pdf_info: Optional[Dict] = None


@router.get(
    "/papers/{paper_id}/find-pdf",
    summary="PDF ì°¾ê¸°",
    description="ë…¼ë¬¸ IDë¡œ PDFë¥¼ ì°¾ì•„ì„œ ë‹¤ìš´ë¡œë“œ (Semantic Scholar â†’ arXiv â†’ Unpaywall)"
)
async def find_pdf(paper_id: str) -> FindPDFResponse:
    """
    PDF ì°¾ê¸° ë° ë‹¤ìš´ë¡œë“œ (ë‹¤ì¤‘ ì†ŒìŠ¤ ì‹œë„ - ì—°êµ¬ì›ì²˜ëŸ¼!)

    1. Local í™•ì¸
    2. Semantic Scholar
    3. arXiv
    4. Unpaywall
    5. Google Scholar
    6. Google ì¼ë°˜ ê²€ìƒ‰
    """
    try:
        logger.info(f"ğŸ” PDF ì°¾ê¸° ì‹œì‘ (ë‹¤ì¤‘ ì†ŒìŠ¤): {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. Semantic Scholarì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 2. ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ PDF ì°¾ê¸°
        title = paper.get("title", "")
        authors = paper.get("authors", [])
        doi = paper.get("doi")

        external_ids = paper.get("externalIds", {})
        arxiv_id = external_ids.get("ArXiv") if external_ids else None
        semantic_scholar_pdf = paper.get("pdf_url")

        pdf_path, source = pdf_processor.find_pdf_from_multiple_sources(
            paper_id=paper_id,
            title=title,
            authors=authors,
            doi=doi,
            arxiv_id=arxiv_id,
            semantic_scholar_pdf=semantic_scholar_pdf
        )

        if pdf_path:
            logger.info(f"âœ… PDF ì°¾ê¸° ì„±ê³µ: {source}")
            return FindPDFResponse(
                paper_id=paper_id,
                pdf_found=True,
                pdf_url=semantic_scholar_pdf if source == "semantic_scholar" else None,
                source=source,
                local_path=str(pdf_path)
            )

        # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì‹¤íŒ¨
        logger.warning(f"âš ï¸ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ PDF ì°¾ê¸° ì‹¤íŒ¨: {paper_id}")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=False,
            pdf_url=None,
            source=None,
            local_path=None
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ PDF ì°¾ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "FIND_PDF_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/pdf-text",
    summary="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ",
    description="PDFì—ì„œ ì „ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (LLM ìš”ì•½ìš©)"
)
async def extract_pdf_text(paper_id: str) -> ExtractTextResponse:
    """
    PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ

    1. ë¡œì»¬ì— PDF ìˆëŠ”ì§€ í™•ì¸
    2. ì—†ìœ¼ë©´ find-pdf ë¨¼ì € í˜¸ì¶œ
    3. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        logger.info(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {paper_id}")

        pdf_processor = PDFProcessor()

        # 1. PDF ê²½ë¡œ í™•ì¸
        pdf_path = pdf_processor.get_pdf_path(paper_id)

        # 2. PDF ì—†ìœ¼ë©´ ì°¾ê¸°
        if not pdf_path:
            logger.info(f"ğŸ” PDF ì—†ìŒ, ì°¾ê¸° ì‹œë„...")
            find_result = await find_pdf(paper_id)
            if not find_result.pdf_found or not find_result.local_path:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail={
                        "error": "PDF_NOT_FOUND",
                        "message": f"PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}",
                        "suggestion": "Abstractë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ PDF ì¶”ê°€ í•„ìš”"
                    }
                )
            pdf_path = Path(find_result.local_path)

        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = pdf_processor.extract_text_from_pdf(pdf_path)
        if not text:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={
                    "error": "TEXT_EXTRACTION_FAILED",
                    "message": "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ìˆ˜ ìˆìŒ)"
                }
            )

        # 4. PDF ë©”íƒ€ë°ì´í„°
        pdf_info = pdf_processor.get_pdf_info(pdf_path)

        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} chars, {pdf_info.get('num_pages', 0)} pages")

        return ExtractTextResponse(
            paper_id=paper_id,
            text_length=len(text),
            num_pages=pdf_info.get("num_pages"),
            text=text,
            pdf_info=pdf_info
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "EXTRACT_TEXT_ERROR", "message": str(e)}
        )


@router.post(
    "/papers/summary",
    summary="ë…¼ë¬¸ ìš”ì•½ ì €ì¥",
    description="LLMì´ ìƒì„±í•œ ë…¼ë¬¸ ìš”ì•½ ì €ì¥"
)
async def save_paper_summary(request: PaperSummaryRequest):
    """
    ë…¼ë¬¸ ìš”ì•½ ì €ì¥

    data/paper_summaries/{paper_id}.jsonì— ì €ì¥
    """
    try:
        logger.info(f"ğŸ’¾ ìš”ì•½ ì €ì¥ ì‹œì‘: {request.paper_id}")

        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        summary_dir = Path("data/paper_summaries")
        summary_dir.mkdir(parents=True, exist_ok=True)

        # ìš”ì•½ ë°ì´í„°
        summary_data = {
            "paper_id": request.paper_id,
            "summary": request.summary,
            "metadata": request.metadata or {},
            "created_at": datetime.now().isoformat(),
            "summary_length": len(request.summary)
        }

        # ì €ì¥
        summary_path = summary_dir / f"{request.paper_id}.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")

        return {
            "message": "ìš”ì•½ ì €ì¥ ì™„ë£Œ",
            "paper_id": request.paper_id,
            "summary_path": str(summary_path),
            "summary_length": len(request.summary)
        }

    except Exception as e:
        logger.error(f"âŒ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "SAVE_SUMMARY_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/summary",
    summary="ë…¼ë¬¸ ìš”ì•½ ì¡°íšŒ",
    description="ì €ì¥ëœ ë…¼ë¬¸ ìš”ì•½ ì¡°íšŒ"
)
async def get_paper_summary(paper_id: str):
    """ì €ì¥ëœ ìš”ì•½ ì¡°íšŒ"""
    try:
        summary_path = Path(f"data/paper_summaries/{paper_id}.json")

        if not summary_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "SUMMARY_NOT_FOUND", "message": f"ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)

        return summary_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "GET_SUMMARY_ERROR", "message": str(e)}
        )


# ============================================================
# ğŸ” ê°œë³„ PDF ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ (n8n ì›Œí¬í”Œë¡œìš°ìš©)
# ============================================================

@router.get(
    "/papers/{paper_id}/try-semantic-scholar",
    summary="[1ë‹¨ê³„] Semantic Scholar PDF ì‹œë„",
    description="Semantic Scholarì—ì„œ PDF ì°¾ê¸°"
)
async def try_semantic_scholar(paper_id: str) -> FindPDFResponse:
    """
    [ì—°êµ¬ì› ì‚¬ê³  1ë‹¨ê³„] Semantic Scholarì—ì„œ PDF ì°¾ê¸°
    """
    try:
        logger.info(f"ğŸ” [1/5] Semantic Scholar ì‹œë„: {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. ë¡œì»¬ì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ì‹¤ì œë¡œ ì½ì„ ìˆ˜ ìˆëŠ”ì§€ë„ í™•ì¸)
        local_path = pdf_processor.get_pdf_path(paper_id)
        if local_path:
            # ì‹¤ì œë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ê°€ëŠ¥í•œì§€ í™•ì¸
            test_text = pdf_processor.extract_text_from_pdf(local_path)
            if test_text and len(test_text.strip()) > 100:
                logger.info(f"âœ… [LOCAL] PDF ì´ë¯¸ ì¡´ì¬í•˜ê³  ì½ê¸° ê°€ëŠ¥")
                return FindPDFResponse(
                    paper_id=paper_id,
                    pdf_found=True,
                    source="local",
                    local_path=str(local_path)
                )
            else:
                logger.warning(f"âš ï¸ [LOCAL] PDF íŒŒì¼ì€ ìˆì§€ë§Œ ì½ì„ ìˆ˜ ì—†ìŒ, ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„")
                # ì†ìƒëœ íŒŒì¼ ì‚­ì œ
                local_path.unlink(missing_ok=True)

        # 2. Semantic Scholarì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 3. Semantic Scholar PDF URL í™•ì¸
        pdf_url = paper.get("pdf_url")
        if not pdf_url:
            logger.warning(f"âš ï¸ Semantic Scholarì— PDF ì—†ìŒ")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PDF_NOT_FOUND", "message": "Semantic Scholarì— PDF ì—†ìŒ"}
            )

        # 4. PDF ë‹¤ìš´ë¡œë“œ
        pdf_path = pdf_processor.download_pdf_from_url(paper_id, pdf_url)
        if not pdf_path:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={"error": "DOWNLOAD_FAILED", "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"}
            )

        logger.info(f"âœ… [Semantic Scholar] PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=True,
            pdf_url=pdf_url,
            source="semantic_scholar",
            local_path=str(pdf_path)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Semantic Scholar ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "SEMANTIC_SCHOLAR_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/try-arxiv",
    summary="[2ë‹¨ê³„] arXiv PDF ì‹œë„",
    description="arXivì—ì„œ PDF ì°¾ê¸°"
)
async def try_arxiv(paper_id: str) -> FindPDFResponse:
    """
    [ì—°êµ¬ì› ì‚¬ê³  2ë‹¨ê³„] arXivì—ì„œ PDF ì°¾ê¸°
    """
    try:
        logger.info(f"ğŸ” [2/5] arXiv ì‹œë„: {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ (arXiv ID í•„ìš”)
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 2. arXiv ID í™•ì¸
        external_ids = paper.get("externalIds", {})
        arxiv_id = external_ids.get("ArXiv") if external_ids else None

        if not arxiv_id:
            logger.warning(f"âš ï¸ arXiv ID ì—†ìŒ")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "ARXIV_ID_NOT_FOUND", "message": "arXiv ID ì—†ìŒ"}
            )

        # 3. arXiv PDF URL ì°¾ê¸°
        pdf_url = pdf_processor.find_pdf_from_arxiv(arxiv_id)
        if not pdf_url:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PDF_NOT_FOUND", "message": "arXivì— PDF ì—†ìŒ"}
            )

        # 4. PDF ë‹¤ìš´ë¡œë“œ
        pdf_path = pdf_processor.download_pdf_from_url(paper_id, pdf_url)
        if not pdf_path:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={"error": "DOWNLOAD_FAILED", "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"}
            )

        logger.info(f"âœ… [arXiv] PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=True,
            pdf_url=pdf_url,
            source="arxiv",
            local_path=str(pdf_path)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ arXiv ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "ARXIV_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/try-unpaywall",
    summary="[3ë‹¨ê³„] Unpaywall PDF ì‹œë„",
    description="DOIë¡œ Unpaywallì—ì„œ PDF ì°¾ê¸°"
)
async def try_unpaywall(paper_id: str) -> FindPDFResponse:
    """
    [ì—°êµ¬ì› ì‚¬ê³  3ë‹¨ê³„] Unpaywallì—ì„œ PDF ì°¾ê¸°
    """
    try:
        logger.info(f"ğŸ” [3/5] Unpaywall ì‹œë„: {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ (DOI í•„ìš”)
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 2. DOI í™•ì¸
        doi = paper.get("doi")
        if not doi:
            logger.warning(f"âš ï¸ DOI ì—†ìŒ")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "DOI_NOT_FOUND", "message": "DOI ì—†ìŒ"}
            )

        # 3. Unpaywall PDF URL ì°¾ê¸°
        pdf_url = pdf_processor.find_pdf_from_doi(doi)
        if not pdf_url:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PDF_NOT_FOUND", "message": "Unpaywallì— PDF ì—†ìŒ"}
            )

        # 4. PDF ë‹¤ìš´ë¡œë“œ
        pdf_path = pdf_processor.download_pdf_from_url(paper_id, pdf_url)
        if not pdf_path:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={"error": "DOWNLOAD_FAILED", "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"}
            )

        logger.info(f"âœ… [Unpaywall] PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=True,
            pdf_url=pdf_url,
            source="unpaywall",
            local_path=str(pdf_path)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Unpaywall ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "UNPAYWALL_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/try-google-scholar",
    summary="[4ë‹¨ê³„] Google Scholar PDF ì‹œë„",
    description="Google Scholarì—ì„œ PDF ì°¾ê¸°"
)
async def try_google_scholar(paper_id: str) -> FindPDFResponse:
    """
    [ì—°êµ¬ì› ì‚¬ê³  4ë‹¨ê³„] Google Scholarì—ì„œ PDF ì°¾ê¸°
    """
    try:
        logger.info(f"ğŸ” [4/5] Google Scholar ì‹œë„: {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ (ì œëª©, ì €ì í•„ìš”)
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 2. Google Scholar ê²€ìƒ‰
        title = paper.get("title", "")
        authors = paper.get("authors", [])

        pdf_url = pdf_processor.search_google_scholar(title, authors)
        if not pdf_url:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PDF_NOT_FOUND", "message": "Google Scholarì— PDF ì—†ìŒ"}
            )

        # 3. PDF ë‹¤ìš´ë¡œë“œ
        pdf_path = pdf_processor.download_pdf_from_url(paper_id, pdf_url)
        if not pdf_path:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={"error": "DOWNLOAD_FAILED", "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"}
            )

        logger.info(f"âœ… [Google Scholar] PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=True,
            pdf_url=pdf_url,
            source="google_scholar",
            local_path=str(pdf_path)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Google Scholar ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "GOOGLE_SCHOLAR_ERROR", "message": str(e)}
        )


@router.get(
    "/papers/{paper_id}/try-google-search",
    summary="[5ë‹¨ê³„] Google ê²€ìƒ‰ PDF ì‹œë„",
    description="Google ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ PDF ì°¾ê¸°"
)
async def try_google_search(paper_id: str) -> FindPDFResponse:
    """
    [ì—°êµ¬ì› ì‚¬ê³  5ë‹¨ê³„] Google ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ PDF ì°¾ê¸°
    """
    try:
        logger.info(f"ğŸ” [5/5] Google ê²€ìƒ‰ ì‹œë„: {paper_id}")

        pdf_processor = PDFProcessor()
        semantic_service = SemanticScholarService()

        # 1. ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ (ì œëª© í•„ìš”)
        paper = semantic_service.get_paper_details(paper_id, download_pdf=False)
        if not paper:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PAPER_NOT_FOUND", "message": f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}"}
            )

        # 2. Google ê²€ìƒ‰
        title = paper.get("title", "")
        pdf_url = pdf_processor.search_by_title_google(title)
        if not pdf_url:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "PDF_NOT_FOUND", "message": "Google ê²€ìƒ‰ì—ì„œ PDF ëª» ì°¾ìŒ"}
            )

        # 3. PDF ë‹¤ìš´ë¡œë“œ
        pdf_path = pdf_processor.download_pdf_from_url(paper_id, pdf_url)
        if not pdf_path:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail={"error": "DOWNLOAD_FAILED", "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"}
            )

        logger.info(f"âœ… [Google Search] PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return FindPDFResponse(
            paper_id=paper_id,
            pdf_found=True,
            pdf_url=pdf_url,
            source="google_search",
            local_path=str(pdf_path)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Google ê²€ìƒ‰ ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "GOOGLE_SEARCH_ERROR", "message": str(e)}
        )

