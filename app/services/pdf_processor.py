"""
PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤
PDF ë‹¤ìš´ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ, ë©”íƒ€ë°ì´í„° íŒŒì‹±
"""
from typing import Optional, Dict
from pathlib import Path
import requests
import PyPDF2
import pdfplumber
from app.utils.logger import get_logger

logger = get_logger()


class PDFProcessor:
    """PDF ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, pdf_dir: str = "data/papers_pdf"):
        """
        ì´ˆê¸°í™”

        Args:
            pdf_dir: PDF ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.pdf_dir = Path(pdf_dir)
        self.pdf_dir.mkdir(parents=True, exist_ok=True)

    def extract_text_from_pdf(self, pdf_path: Path) -> Optional[str]:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            logger.info(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {pdf_path.name}")

            # ë°©ë²• 1: pdfplumber (ë” ì •í™•í•¨)
            try:
                text = self._extract_with_pdfplumber(pdf_path)
                if text and len(text.strip()) > 100:
                    logger.info(f"âœ… pdfplumberë¡œ ì¶”ì¶œ ì™„ë£Œ: {len(text)} chars")
                    return text
            except Exception as e:
                logger.warning(f"pdfplumber ì‹¤íŒ¨: {e}")

            # ë°©ë²• 2: PyPDF2 (fallback)
            try:
                text = self._extract_with_pypdf2(pdf_path)
                if text and len(text.strip()) > 100:
                    logger.info(f"âœ… PyPDF2ë¡œ ì¶”ì¶œ ì™„ë£Œ: {len(text)} chars")
                    return text
            except Exception as e:
                logger.warning(f"PyPDF2 ì‹¤íŒ¨: {e}")

            logger.error("âŒ ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
            return None

        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_with_pdfplumber(self, pdf_path: Path) -> str:
        """pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_parts = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(page_text)
        return "\n\n".join(text_parts)

    def _extract_with_pypdf2(self, pdf_path: Path) -> str:
        """PyPDF2ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_parts = []
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(page_text)
        return "\n\n".join(text_parts)

    def get_pdf_path(self, paper_id: str) -> Optional[Path]:
        """
        ë…¼ë¬¸ IDë¡œ PDF ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ í¬ê¸°ë„ í™•ì¸)

        Args:
            paper_id: ë…¼ë¬¸ ID

        Returns:
            PDF ê²½ë¡œ (ì—†ê±°ë‚˜ ì†ìƒë˜ë©´ None)
        """
        pdf_path = self.pdf_dir / f"{paper_id}.pdf"
        if pdf_path.exists():
            # íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œ 1KB ì´ìƒ)
            file_size = pdf_path.stat().st_size
            if file_size > 1024:
                return pdf_path
            else:
                logger.warning(f"âš ï¸ PDF íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ (ì†ìƒ ê°€ëŠ¥): {file_size} bytes")
                return None
        return None

    def download_pdf_from_url(self, paper_id: str, pdf_url: str) -> Optional[Path]:
        """
        URLì—ì„œ PDF ë‹¤ìš´ë¡œë“œ

        Args:
            paper_id: ë…¼ë¬¸ ID
            pdf_url: PDF URL

        Returns:
            ì €ì¥ëœ PDF ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            pdf_path = self.pdf_dir / f"{paper_id}.pdf"

            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
            if pdf_path.exists():
                logger.info(f"ğŸ“„ PDF ì´ë¯¸ ì¡´ì¬: {pdf_path.name}")
                return pdf_path

            logger.info(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {pdf_url}")
            response = requests.get(pdf_url, timeout=60, stream=True)
            response.raise_for_status()

            # PDF ì €ì¥
            with open(pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            file_size = pdf_path.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_path.name} ({file_size:.2f} MB)")
            return pdf_path

        except Exception as e:
            logger.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({paper_id}): {e}")
            return None

    def find_pdf_from_arxiv(self, arxiv_id: str) -> Optional[str]:
        """
        arXiv IDë¡œ PDF URL ì°¾ê¸°

        Args:
            arxiv_id: arXiv ID (ì˜ˆ: 2301.12345)

        Returns:
            PDF URL (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # arXiv ID ì •ê·œí™”
            arxiv_id = arxiv_id.replace("arXiv:", "").strip()
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"

            # HEAD ìš”ì²­ìœ¼ë¡œ ì¡´ì¬ í™•ì¸
            response = requests.head(pdf_url, timeout=10)
            if response.status_code == 200:
                logger.info(f"âœ… arXiv PDF ë°œê²¬: {pdf_url}")
                return pdf_url

            logger.warning(f"âš ï¸ arXiv PDF ì—†ìŒ: {arxiv_id}")
            return None

        except Exception as e:
            logger.error(f"âŒ arXiv PDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def find_pdf_from_doi(self, doi: str) -> Optional[str]:
        """
        DOIë¡œ Unpaywall APIë¥¼ í†µí•´ PDF URL ì°¾ê¸°

        Args:
            doi: DOI (ì˜ˆ: 10.1145/3589334.3645598)

        Returns:
            PDF URL (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # Unpaywall API (ë¬´ë£Œ, Open Access PDFë§Œ)
            email = "research@example.com"  # í•„ìˆ˜ (ì •ì±…ìƒ)
            api_url = f"https://api.unpaywall.org/v2/{doi}?email={email}"

            response = requests.get(api_url, timeout=10)
            if response.status_code == 200:
                data = response.json()

                # Open Access PDF URL ì°¾ê¸°
                if data.get("is_oa") and data.get("best_oa_location"):
                    pdf_url = data["best_oa_location"].get("url_for_pdf")
                    if pdf_url:
                        logger.info(f"âœ… Unpaywall PDF ë°œê²¬: {pdf_url}")
                        return pdf_url

            logger.warning(f"âš ï¸ Unpaywall PDF ì—†ìŒ: {doi}")
            return None

        except Exception as e:
            logger.error(f"âŒ Unpaywall ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def search_google_scholar(self, title: str, authors: Optional[list] = None) -> Optional[str]:
        """
        Google Scholarì—ì„œ PDF ë§í¬ ì°¾ê¸°

        Args:
            title: ë…¼ë¬¸ ì œëª©
            authors: ì €ì ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)

        Returns:
            PDF URL (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            import urllib.parse
            from bs4 import BeautifulSoup

            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            query = title
            if authors and len(authors) > 0:
                query += f" {authors[0]}"

            encoded_query = urllib.parse.quote(query)
            search_url = f"https://scholar.google.com/scholar?q={encoded_query}"

            # User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }

            response = requests.get(search_url, headers=headers, timeout=15)
            if response.status_code != 200:
                logger.warning(f"âš ï¸ Google Scholar ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # PDF ë§í¬ ì°¾ê¸° ([PDF] ë§í¬)
            for link in soup.find_all('a'):
                href = link.get('href', '')
                text = link.get_text()

                if '[PDF]' in text and href and isinstance(href, str):
                    logger.info(f"âœ… Google Scholar PDF ë°œê²¬: {href}")
                    return href

            logger.warning(f"âš ï¸ Google Scholarì—ì„œ PDF ëª» ì°¾ìŒ: {title[:50]}...")
            return None

        except Exception as e:
            logger.error(f"âŒ Google Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def search_by_title_google(self, title: str) -> Optional[str]:
        """
        êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ë…¼ë¬¸ ì œëª© + PDF ê²€ìƒ‰

        Args:
            title: ë…¼ë¬¸ ì œëª©

        Returns:
            PDF URL (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            import urllib.parse
            from bs4 import BeautifulSoup

            # "ë…¼ë¬¸ì œëª© pdf" ê²€ìƒ‰
            query = f'"{title}" filetype:pdf'
            encoded_query = urllib.parse.quote(query)
            search_url = f"https://www.google.com/search?q={encoded_query}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }

            response = requests.get(search_url, headers=headers, timeout=15)
            if response.status_code != 200:
                logger.warning(f"âš ï¸ Google ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # PDF ë§í¬ ì°¾ê¸°
            for link in soup.find_all('a'):
                href = link.get('href', '')
                if href and isinstance(href, str) and '.pdf' in href.lower() and 'http' in href:
                    # URL ì •ì œ
                    if 'url?q=' in href:
                        pdf_url = href.split('url?q=')[1].split('&')[0]
                        pdf_url = urllib.parse.unquote(pdf_url)
                    else:
                        pdf_url = href

                    logger.info(f"âœ… Google ê²€ìƒ‰ìœ¼ë¡œ PDF ë°œê²¬: {pdf_url}")
                    return pdf_url

            logger.warning(f"âš ï¸ Google ê²€ìƒ‰ì—ì„œ PDF ëª» ì°¾ìŒ: {title[:50]}...")
            return None

        except Exception as e:
            logger.error(f"âŒ Google ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def find_pdf_from_multiple_sources(
        self,
        paper_id: str,
        title: str,
        authors: Optional[list] = None,
        doi: Optional[str] = None,
        arxiv_id: Optional[str] = None,
        semantic_scholar_pdf: Optional[str] = None
    ) -> tuple[Optional[Path], Optional[str]]:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ PDF ì°¾ê¸° (ì—°êµ¬ì›ì²˜ëŸ¼ ëê¹Œì§€ ì‹œë„!)

        Args:
            paper_id: ë…¼ë¬¸ ID
            title: ë…¼ë¬¸ ì œëª©
            authors: ì €ì ë¦¬ìŠ¤íŠ¸
            doi: DOI
            arxiv_id: arXiv ID
            semantic_scholar_pdf: Semantic Scholar PDF URL

        Returns:
            (PDF ê²½ë¡œ, ì†ŒìŠ¤ëª…) íŠœí”Œ
        """
        logger.info(f"ğŸ” PDF ì°¾ê¸° ì‹œì‘ (ë‹¤ì¤‘ ì†ŒìŠ¤): {title[:50]}...")

        # 1. ì´ë¯¸ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸
        local_path = self.get_pdf_path(paper_id)
        if local_path:
            logger.info(f"âœ… [LOCAL] PDF ì´ë¯¸ ì¡´ì¬")
            return local_path, "local"

        # 2. Semantic Scholar PDF
        if semantic_scholar_pdf:
            logger.info(f"ğŸ” [1/5] Semantic Scholar ì‹œë„...")
            pdf_path = self.download_pdf_from_url(paper_id, semantic_scholar_pdf)
            if pdf_path:
                return pdf_path, "semantic_scholar"

        # 3. arXiv
        if arxiv_id:
            logger.info(f"ğŸ” [2/5] arXiv ì‹œë„...")
            pdf_url = self.find_pdf_from_arxiv(arxiv_id)
            if pdf_url:
                pdf_path = self.download_pdf_from_url(paper_id, pdf_url)
                if pdf_path:
                    return pdf_path, "arxiv"

        # 4. Unpaywall (DOI)
        if doi:
            logger.info(f"ğŸ” [3/5] Unpaywall ì‹œë„...")
            pdf_url = self.find_pdf_from_doi(doi)
            if pdf_url:
                pdf_path = self.download_pdf_from_url(paper_id, pdf_url)
                if pdf_path:
                    return pdf_path, "unpaywall"

        # 5. Google Scholar
        logger.info(f"ğŸ” [4/5] Google Scholar ì‹œë„...")
        pdf_url = self.search_google_scholar(title, authors)
        if pdf_url:
            pdf_path = self.download_pdf_from_url(paper_id, pdf_url)
            if pdf_path:
                return pdf_path, "google_scholar"

        # 6. Google ì¼ë°˜ ê²€ìƒ‰
        logger.info(f"ğŸ” [5/5] Google ê²€ìƒ‰ ì‹œë„...")
        pdf_url = self.search_by_title_google(title)
        if pdf_url:
            pdf_path = self.download_pdf_from_url(paper_id, pdf_url)
            if pdf_path:
                return pdf_path, "google_search"

        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        logger.error(f"âŒ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ PDF ì°¾ê¸° ì‹¤íŒ¨: {title[:50]}...")
        return None, None

    def get_pdf_info(self, pdf_path: Path) -> Dict:
        """
        PDF ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Args:
            pdf_path: PDF ê²½ë¡œ

        Returns:
            ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                info = reader.metadata or {}

                return {
                    "num_pages": len(reader.pages),
                    "title": info.get("/Title", ""),
                    "author": info.get("/Author", ""),
                    "subject": info.get("/Subject", ""),
                    "creator": info.get("/Creator", ""),
                    "producer": info.get("/Producer", ""),
                }
        except Exception as e:
            logger.error(f"PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

