"""
Semantic Scholar API í´ë¼ì´ì–¸íŠ¸
ê³µì‹ REST APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤
"""
from typing import List, Dict, Optional
import requests
from app.utils.logger import get_logger, log_execution_time
from app.config import settings
import time
import json
import os
from pathlib import Path
from datetime import datetime

logger = get_logger()


class SemanticScholarService:
    """Semantic Scholar API ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    BASE_URL = "https://api.semanticscholar.org/graph/v1"

    def __init__(self, api_key: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            api_key: Semantic Scholar API Key (ì„ íƒ)
                    Noneì´ë©´ í™˜ê²½ë³€ìˆ˜(.env)ì—ì„œ ìë™ìœ¼ë¡œ ì½ìŒ
                    ì—†ìœ¼ë©´: 100 requests/5ë¶„
                    ìˆìœ¼ë©´: 5,000 requests/5ë¶„
        """
        # API Key ìš°ì„ ìˆœìœ„: íŒŒë¼ë¯¸í„° > í™˜ê²½ë³€ìˆ˜
        self.api_key = api_key or settings.SEMANTIC_SCHOLAR_API_KEY
        self.session = requests.Session()

        # ê¸°ë³¸ í—¤ë” ì„¤ì • (User-Agent í•„ìˆ˜!)
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "application/json"
        })

        # API Keyê°€ ìˆìœ¼ë©´ í—¤ë”ì— ì¶”ê°€
        if self.api_key:
            self.session.headers.update({"x-api-key": self.api_key})
            logger.info("âœ… API Keyë¡œ ì´ˆê¸°í™” (5,000 req/5min) ğŸš€")
        else:
            logger.info("âœ… ë¬´ë£Œ ë²„ì „ìœ¼ë¡œ ì´ˆê¸°í™” (100 req/5min) âš ï¸")

        # ìºì‹œ ë° PDF ì €ì¥ ê²½ë¡œ ì„¤ì •
        self.cache_file = Path("data/papers_cache.json")
        self.pdf_dir = Path("data/papers_pdf")
        self._ensure_directories()

    @log_execution_time
    def search_papers(
        self,
        keyword: str,
        year_from: int = 2024,
        year_to: int = 2025,
        limit: int = 5,
        fields: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            year_from: ì‹œì‘ ì—°ë„
            year_to: ì¢…ë£Œ ì—°ë„
            limit: ë°˜í™˜í•  ë…¼ë¬¸ ìˆ˜ (ìµœëŒ€ 100)
            fields: ê°€ì ¸ì˜¬ í•„ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: keyword='{keyword}', year={year_from}-{year_to}")

            # ê¸°ë³¸ í•„ë“œ ì„¤ì •
            if fields is None:
                fields = [
                    "paperId",
                    "title",
                    "authors",
                    "year",
                    "venue",
                    "citationCount",
                    "url",
                    "abstract",
                    "externalIds",
                    "openAccessPdf"
                ]

            # API ì—”ë“œí¬ì¸íŠ¸
            url = f"{self.BASE_URL}/paper/search"

            # íŒŒë¼ë¯¸í„° ì„¤ì •
            params = {
                "query": keyword,
                "year": f"{year_from}-{year_to}",
                "limit": min(limit, 100),  # ìµœëŒ€ 100
                "fields": ",".join(fields)
            }

            # API ìš”ì²­ (ì¬ì‹œë„ ë¡œì§)
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(url, params=params, timeout=30)

                    # 429 Rate Limit ì²˜ë¦¬
                    if response.status_code == 429:
                        wait_time = (attempt + 1) * 10  # 10ì´ˆ, 20ì´ˆ, 30ì´ˆ
                        logger.warning(f"â³ Rate Limit ë„ë‹¬. {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue

                    response.raise_for_status()
                    break  # ì„±ê³µí•˜ë©´ ì¢…ë£Œ

                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                        raise
                    logger.warning(f"ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                    time.sleep(5)

            data = response.json()
            papers = data.get("data", [])

            if not papers:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # ë…¼ë¬¸ ì •ë³´ ë³€í™˜
            result_papers = []
            for paper in papers:
                try:
                    converted = self._convert_paper_format(paper)
                    result_papers.append(converted)
                    logger.info(f"âœ… [{len(result_papers)}] {converted['title'][:60]}... (ì¸ìš©: {converted['citations']})")
                except Exception as e:
                    logger.warning(f"ë…¼ë¬¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            # Citation ìˆ˜ ê¸°ì¤€ ì •ë ¬
            result_papers.sort(key=lambda x: x.get('citations', 0), reverse=True)

            logger.info(f"ğŸ‰ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(result_papers)}ê°œ ë…¼ë¬¸")
            return result_papers

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise Exception(f"Semantic Scholar API ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    def _convert_paper_format(self, paper: Dict) -> Dict:
        """Semantic Scholar ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

        # ì €ì ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        authors = []
        for author in paper.get("authors", []):
            if author.get("name"):
                authors.append(author["name"])

        # DOI ì¶”ì¶œ
        external_ids = paper.get("externalIds", {})
        doi = external_ids.get("DOI")

        # PDF URL ì¶”ì¶œ
        pdf_url = None
        open_access = paper.get("openAccessPdf")
        if open_access and open_access.get("url"):
            pdf_url = open_access["url"]

        return {
            "id": paper.get("paperId", ""),
            "title": paper.get("title", ""),
            "authors": authors,
            "year": paper.get("year"),
            "venue": paper.get("venue", ""),
            "citations": paper.get("citationCount", 0),
            "url": paper.get("url", ""),
            "abstract": paper.get("abstract") or None,  # Noneì„ ëª…ì‹œì ìœ¼ë¡œ í—ˆìš©
            "doi": doi,
            "pdf_url": pdf_url,
        }

    def get_paper_by_id(self, paper_id: str, fields: Optional[List[str]] = None) -> Optional[Dict]:
        """
        ë…¼ë¬¸ IDë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ

        Args:
            paper_id: Semantic Scholar Paper ID
            fields: ê°€ì ¸ì˜¬ í•„ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë…¼ë¬¸ ì •ë³´
        """
        try:
            logger.info(f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ: {paper_id}")

            # ê¸°ë³¸ í•„ë“œ
            if fields is None:
                fields = [
                    "paperId", "title", "authors", "year", "venue",
                    "citationCount", "url", "abstract", "externalIds",
                    "openAccessPdf", "references", "citations"
                ]

            # API ìš”ì²­
            url = f"{self.BASE_URL}/paper/{paper_id}"
            params = {"fields": ",".join(fields)}

            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()

            paper = response.json()
            return self._convert_paper_format(paper)

        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def bulk_search_papers(
        self,
        keywords: List[str],
        year_from: int = 2024,
        year_to: int = 2025,
        limit_per_keyword: int = 5
    ) -> Dict[str, List[Dict]]:
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë™ì‹œ ê²€ìƒ‰

        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            year_from: ì‹œì‘ ì—°ë„
            year_to: ì¢…ë£Œ ì—°ë„
            limit_per_keyword: í‚¤ì›Œë“œë‹¹ ë…¼ë¬¸ ìˆ˜

        Returns:
            í‚¤ì›Œë“œë³„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        results = {}

        for keyword in keywords:
            try:
                papers = self.search_papers(
                    keyword=keyword,
                    year_from=year_from,
                    year_to=year_to,
                    limit=limit_per_keyword
                )
                results[keyword] = papers

                # Rate Limiting ë°©ì§€ (ë¬´ë£Œ: 100 req/5min)
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                results[keyword] = []

        return results

    @log_execution_time
    def get_citation_network(
        self,
        paper_id: str,
        max_references: int = 20,
        max_citations: int = 20,
        include_references: bool = True,
        include_citations: bool = True
    ) -> Dict:
        """
        ë…¼ë¬¸ì˜ Citation Network ê°€ì ¸ì˜¤ê¸° (References + Citations)

        Args:
            paper_id: Seed ë…¼ë¬¸ ID
            max_references: ìµœëŒ€ References ìˆ˜
            max_citations: ìµœëŒ€ Citations ìˆ˜
            include_references: References í¬í•¨ ì—¬ë¶€
            include_citations: Citations í¬í•¨ ì—¬ë¶€

        Returns:
            {
                "seed_paper": {...},
                "references": [...],
                "citations": [...],
                "total_references": int,
                "total_citations": int
            }
        """
        try:
            logger.info(f"ğŸŒ³ Citation Network êµ¬ì¶• ì‹œì‘: {paper_id}")

            # 1. Seed ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (References + Citations í¬í•¨)
            fields = [
                "paperId", "title", "authors", "year", "venue",
                "citationCount", "url", "abstract", "externalIds",
                "openAccessPdf"
            ]

            if include_references:
                fields.append("references")
            if include_citations:
                fields.append("citations")

            url = f"{self.BASE_URL}/paper/{paper_id}"
            params = {"fields": ",".join(fields)}

            # API ìš”ì²­ (ì¬ì‹œë„ ë¡œì§)
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(url, params=params, timeout=30)

                    # 429 Rate Limit ì²˜ë¦¬
                    if response.status_code == 429:
                        wait_time = (attempt + 1) * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ
                        logger.warning(f"â³ Rate Limit ë„ë‹¬. {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue

                    response.raise_for_status()
                    break  # ì„±ê³µí•˜ë©´ ì¢…ë£Œ

                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                        raise
                    logger.warning(f"Citation Network ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                    time.sleep(3)

            seed_data = response.json()

            # 2. Seed ë…¼ë¬¸ ë³€í™˜
            seed_paper = self._convert_paper_format(seed_data)

            # 3. References ì²˜ë¦¬ (ìƒì„¸ ì •ë³´ + PDF ë‹¤ìš´ë¡œë“œ)
            references = []
            total_references = 0
            if include_references and "references" in seed_data:
                raw_references = seed_data["references"]
                total_references = len(raw_references)

                logger.info(f"ğŸ”„ References ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘... (ìµœëŒ€ {max_references}ê°œ)")
                for idx, ref in enumerate(raw_references[:max_references], 1):
                    try:
                        if ref.get("paperId"):
                            # ìƒì„¸ ì •ë³´ ì¡°íšŒ (ìºì‹± + PDF ë‹¤ìš´ë¡œë“œ)
                            detailed_paper = self.get_paper_details(ref["paperId"], download_pdf=True)
                            if detailed_paper:
                                references.append(detailed_paper)
                                logger.info(f"  [{idx}/{max_references}] âœ… {detailed_paper['title'][:50]}...")
                            else:
                                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë¼ë„ ì €ì¥
                                converted = self._convert_citation_item(ref)
                                references.append(converted)
                                logger.warning(f"  [{idx}/{max_references}] âš ï¸ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥")

                            # Rate Limit ë°©ì§€
                            time.sleep(0.5)
                    except Exception as e:
                        logger.warning(f"Reference ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

                logger.info(f"âœ… References: {len(references)}/{total_references}ê°œ ì™„ë£Œ")

            # 4. Citations ì²˜ë¦¬ (ìƒì„¸ ì •ë³´ + PDF ë‹¤ìš´ë¡œë“œ)
            citations = []
            total_citations = 0
            if include_citations and "citations" in seed_data:
                raw_citations = seed_data["citations"]
                total_citations = len(raw_citations)

                logger.info(f"ğŸ”„ Citations ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘... (ìµœëŒ€ {max_citations}ê°œ)")
                for idx, cit in enumerate(raw_citations[:max_citations], 1):
                    try:
                        if cit.get("paperId"):
                            # ìƒì„¸ ì •ë³´ ì¡°íšŒ (ìºì‹± + PDF ë‹¤ìš´ë¡œë“œ)
                            detailed_paper = self.get_paper_details(cit["paperId"], download_pdf=True)
                            if detailed_paper:
                                citations.append(detailed_paper)
                                logger.info(f"  [{idx}/{max_citations}] âœ… {detailed_paper['title'][:50]}...")
                            else:
                                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë¼ë„ ì €ì¥
                                converted = self._convert_citation_item(cit)
                                citations.append(converted)
                                logger.warning(f"  [{idx}/{max_citations}] âš ï¸ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥")

                            # Rate Limit ë°©ì§€
                            time.sleep(0.5)
                    except Exception as e:
                        logger.warning(f"Citation ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

                logger.info(f"âœ… Citations: {len(citations)}/{total_citations}ê°œ ì™„ë£Œ")

            logger.info(f"ğŸ‰ Citation Network êµ¬ì¶• ì™„ë£Œ: Seed 1ê°œ + Ref {len(references)}ê°œ + Cit {len(citations)}ê°œ")

            return {
                "seed_paper": seed_paper,
                "references": references,
                "citations": citations,
                "total_references": total_references,
                "total_citations": total_citations
            }

        except Exception as e:
            logger.error(f"âŒ Citation Network êµ¬ì¶• ì‹¤íŒ¨: {e}")
            raise

    def _convert_citation_item(self, item: Dict) -> Dict:
        """Citation/Reference ì•„ì´í…œì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        authors = []
        for author in item.get("authors", []):
            if author.get("name"):
                authors.append(author["name"])

        external_ids = item.get("externalIds", {})
        doi = external_ids.get("DOI") if external_ids else None

        return {
            "id": item.get("paperId", ""),
            "title": item.get("title", ""),
            "authors": authors,
            "year": item.get("year"),
            "venue": item.get("venue", ""),
            "citations": item.get("citationCount", 0),
            "url": item.get("url", ""),
            "abstract": item.get("abstract"),
            "doi": doi,
            "pdf_url": None,  # Citation APIì—ì„œëŠ” ì œê³µ ì•ˆë¨
        }

    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        self.pdf_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)

    def _load_cache(self) -> Dict:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if not self.cache_file.exists():
            return {"papers": {}, "last_updated": None, "cache_info": {"total_papers": 0, "total_pdfs": 0}}

        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"papers": {}, "last_updated": None, "cache_info": {"total_papers": 0, "total_pdfs": 0}}

    def _save_cache(self, cache: Dict):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            cache["last_updated"] = datetime.now().isoformat()
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_paper_details(self, paper_id: str, download_pdf: bool = True) -> Optional[Dict]:
        """
        ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ ë° PDF ë‹¤ìš´ë¡œë“œ (ìºì‹± í¬í•¨)

        Args:
            paper_id: Semantic Scholar Paper ID
            download_pdf: PDF ë‹¤ìš´ë¡œë“œ ì—¬ë¶€

        Returns:
            ë…¼ë¬¸ ìƒì„¸ ì •ë³´ (ìºì‹œ ë˜ëŠ” API ì¡°íšŒ)
        """
        try:
            # 1. ìºì‹œ í™•ì¸
            cache = self._load_cache()
            if paper_id in cache["papers"]:
                logger.info(f"ğŸ“¦ ìºì‹œì—ì„œ ë¡œë“œ: {paper_id}")
                return cache["papers"][paper_id]

            # 2. API ì¡°íšŒ
            logger.info(f"ğŸ” API ì¡°íšŒ ì‹œì‘: {paper_id}")
            fields = [
                "paperId", "title", "authors", "year", "venue",
                "citationCount", "url", "abstract", "externalIds",
                "openAccessPdf"
            ]

            url = f"{self.BASE_URL}/paper/{paper_id}"
            params = {"fields": ",".join(fields)}

            # Rate Limit ëŒ€ì‘ ì¬ì‹œë„
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(url, params=params, timeout=30)

                    if response.status_code == 429:
                        wait_time = (attempt + 1) * 5
                        logger.warning(f"â³ Rate Limit. {wait_time}ì´ˆ ëŒ€ê¸°... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue

                    response.raise_for_status()
                    break

                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:
                        raise
                    logger.warning(f"ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                    time.sleep(3)

            paper_data = response.json()
            paper = self._convert_paper_format(paper_data)

            # 3. PDF ë‹¤ìš´ë¡œë“œ
            if download_pdf and paper.get("pdf_url"):
                pdf_path = self._download_pdf(paper_id, paper["pdf_url"])
                if pdf_path:
                    paper["local_pdf_path"] = str(pdf_path)
                    cache["cache_info"]["total_pdfs"] = cache["cache_info"].get("total_pdfs", 0) + 1

            # 4. ìºì‹œ ì €ì¥
            cache["papers"][paper_id] = paper
            cache["cache_info"]["total_papers"] = len(cache["papers"])
            self._save_cache(cache)

            logger.info(f"âœ… ë…¼ë¬¸ ì¡°íšŒ ì™„ë£Œ: {paper['title'][:60]}...")
            return paper

        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ ({paper_id}): {e}")
            return None

    def _download_pdf(self, paper_id: str, pdf_url: str) -> Optional[Path]:
        """
        PDF ë‹¤ìš´ë¡œë“œ

        Args:
            paper_id: ë…¼ë¬¸ ID
            pdf_url: PDF URL

        Returns:
            ì €ì¥ëœ PDF íŒŒì¼ ê²½ë¡œ
        """
        try:
            pdf_path = self.pdf_dir / f"{paper_id}.pdf"

            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
            if pdf_path.exists():
                logger.info(f"ğŸ“„ PDF ì´ë¯¸ ì¡´ì¬: {pdf_path.name}")
                return pdf_path

            logger.info(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {pdf_url}")
            response = self.session.get(pdf_url, timeout=60, stream=True)
            response.raise_for_status()

            # PDF ì €ì¥
            with open(pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            file_size = pdf_path.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_path.name} ({file_size:.2f} MB)")
            return pdf_path

        except Exception as e:
            logger.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({paper_id}): {e}")
            return None

